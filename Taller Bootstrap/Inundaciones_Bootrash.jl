using Statistics
using Random, Distributions, DataFrames, StatsBase, GLM, Plots
using DecisionTree
using KernelDensity
using StatsPlots
using Random, Distributions
# Datos originales: lluvia en 3 d√≠as (mm)
lluvia = [10, 25, 40]
# N√∫mero de r√©plicas en cada remuestreo
n = length(lluvia)
# Generar TODOS los posibles remuestreos con reemplazo
remuestreos = [[lluvia[rand(1:n)] for _ in 1:n] for _ in 1:27]  # 3^3 = 27 casos
# Calcular la media de cada remuestreo
medias = [mean(r) for r in remuestreos]
# Mostrar los primeros remuestreos
for (i, r) in enumerate(remuestreos[1:10])  # mostrar solo 10 de ejemplo
    println("Remuestreo $i: ", r, " ‚Üí media = ", mean(r))
end
println("\nDistribuci√≥n de medias bootstrap:")
println(medias)
println("Media observada (original): ", mean(lluvia))
println("Media promedio de remuestreos: ", mean(medias))





Random.seed!(123)

# Simulamos 20 d√≠as de lluvia (mm)
lluvia = rand(Gamma(2, 15), 20)   # media ‚âà 30 mm

# Media observada
media_obs = mean(lluvia)
println("Media de lluvia observada (20 d√≠as): ", round(media_obs, digits=2), " mm")

# Bootstrap
B = 1000
medias_boot = [mean(sample(lluvia, 20, replace=true)) for _ in 1:B]

# Error est√°ndar bootstrap
se_boot = std(medias_boot)
println("Error est√°ndar bootstrap: ", round(se_boot, digits=2))

# Intervalo de confianza (percentil al 95%)
ic_95 = quantile(medias_boot, [0.025, 0.975])
println("IC 95% bootstrap de la media de lluvia: ", round.(ic_95, digits=2), " mm")



# ================================
# 1. Dataset realista de inundaciones
# ================================
Random.seed!(123)
n = 300

lluvia_total      = rand(Gamma(3, 25), n)                 # media ‚âà 75 mm
intensidad_lluvia = rand(Gamma(2, 15), n)                 # media ‚âà 30 mm/h
duracion_lluvia   = rand(LogNormal(log(2), 0.4), n)       # h
capacidad_drenaje = rand(LogNormal(log(60), 0.3), n)      # l/s
impermeabilidad   = rand(Beta(5, 2), n)                   # tendencia alta

riesgo = (
    0.6 * (lluvia_total / 80) +           # Escala mejorada
    0.5 * (intensidad_lluvia / 60) +      # M√°s peso a intensidad
    0.6 * (impermeabilidad * 1.2) +       # Impermeabilidad escalada
    -0.7 * (capacidad_drenaje / 50) +     # M√°s impacto del drenaje
    0.4 * (duracion_lluvia / 5)           # Duraci√≥n con mejor escala
)

prob_inundacion = 1 ./ (1 .+ exp.(-5 .* (riesgo .- 0.5)))
zona_inundada = Int.(rand.(Bernoulli.(prob_inundacion)))

df = DataFrame(
    lluvia=lluvia_total,
    intensidad=intensidad_lluvia,
    duracion=duracion_lluvia,
    drenaje=capacidad_drenaje,
    impermeabilidad=impermeabilidad,
    zona_inundada=zona_inundada
)

print(df)
# ================================
# 2. Modelos base
# ================================
# Regresi√≥n log√≠stica
modelo_log = glm(@formula(zona_inundada ~ lluvia + intensidad + duracion + drenaje + impermeabilidad),
                 df, Binomial(), LogitLink())
acc_log = mean(round.(GLM.predict(modelo_log)) .== df.zona_inundada)

# √Årbol de decisi√≥n
X_matrix = Matrix(df[:, Not(:zona_inundada)])
y_vector = df.zona_inundada
modelo_tree = DecisionTree.DecisionTreeClassifier(max_depth=4)
DecisionTree.fit!(modelo_tree, X_matrix, y_vector)
pred_tree = DecisionTree.predict(modelo_tree, X_matrix)
acc_tree = mean(pred_tree .== y_vector)

println("Precisi√≥n Log√≠stica: ", acc_log)
println("Precisi√≥n √Årbol: ", acc_tree)

# ================================
# 3. Bootstrap de accuracy
# ================================
n_boot = 3000
accs_log = Float64[]
accs_tree = Float64[]

for b in 1:n_boot
    idxs = sample(1:n, n, replace=true)
    df_b = df[idxs, :]

    # log√≠stica
    mlog_b = glm(@formula(zona_inundada ~ lluvia + intensidad + duracion + drenaje + impermeabilidad),
                 df_b, Binomial(), LogitLink())
    push!(accs_log, mean(round.(GLM.predict(mlog_b)) .== df_b.zona_inundada))

    # √°rbol
    X_b = Matrix(df_b[:, Not(:zona_inundada)])
    y_b = df_b.zona_inundada
    mtree_b = DecisionTree.DecisionTreeClassifier(max_depth=4)
    DecisionTree.fit!(mtree_b, X_b, y_b)
    pred_tb = DecisionTree.predict(mtree_b, X_b)
    push!(accs_tree, mean(pred_tb .== y_b))
end

# Intervalos de confianza
ic_log = quantile(accs_log, [0.025, 0.975])
ic_tree = quantile(accs_tree, [0.025, 0.975])
println("IC Log√≠stica: ", ic_log)
println("IC √Årbol: ", ic_tree)

# ================================
# 4. Visualizaciones bootstrap
# ================================

# Histograma para regresi√≥n log√≠stica con IC
histogram(accs_log, bins=20, alpha=0.5, color=:blue,
    xlabel="Accuracy", ylabel="Frecuencia",
    title="Bootstrap Accuracy - Regresi√≥n Log√≠stica ", 
    legend=false)
vline!([ic_log[1]], line=:dash, color=:red, linewidth=2, label="")
vline!([ic_log[2]], line=:dash, color=:red, linewidth=2, label="")
vline!([mean(accs_log)], line=:solid, color=:black, linewidth=2, label="")

# Histograma para √°rbol de decisi√≥n con IC
histogram(accs_tree, bins=20, alpha=0.5, color=:green,
    xlabel="Accuracy", ylabel="Frecuencia",
    title="Bootstrap Accuracy - √Årbol ", 
    legend=false)
vline!([ic_tree[1]], line=:dash, color=:red, linewidth=2, label="")
vline!([ic_tree[2]], line=:dash, color=:red, linewidth=2, label="")
vline!([mean(accs_tree)], line=:solid, color=:black, linewidth=2, label="")

# Comparaci√≥n con intervalos de confianza
p_comp = histogram(accs_log, bins=20, alpha=0.5, color=:blue,
    label="Log√≠stica", xlabel="Accuracy", ylabel="Frecuencia",
    title="Distribuci√≥n Bootstrap - Comparaci√≥n")
histogram!(accs_tree, bins=20, alpha=0.5, color=:green, label="√Årbol")

# Agregar l√≠neas de IC para log√≠stica
vline!([ic_log[1]], line=:dash, color=:blue, linewidth=1.5, label="IC Log√≠stica")
vline!([ic_log[2]], line=:dash, color=:blue, linewidth=1.5, label="")

# Agregar l√≠neas de IC para √°rbol
vline!([ic_tree[1]], line=:dash, color=:green, linewidth=1.5, label="IC √Årbol")
vline!([ic_tree[2]], line=:dash, color=:green, linewidth=1.5, label="")

# Mostrar el gr√°fico de comparaci√≥n
p_comp

# Boxplot
group_labels = vcat(fill("Log√≠stica", n_boot), fill("√Årbol", n_boot))
acc_values = vcat(accs_log, accs_tree)
boxplot(group_labels, acc_values, legend=false,
    ylabel="Accuracy", title="Boxplot Bootstrap Accuracy")

# KDE
# KDE con intervalos de confianza
density(accs_log, label="Log√≠stica", color=:blue,
    xlabel="Accuracy", ylabel="Densidad",
    title="Distribuciones Bootstrap (KDE)",
    linewidth=2)
density!(accs_tree, label="√Årbol", color=:green, linewidth=2)

# Agregar intervalos de confianza como √°reas sombreadas
vspan!([ic_log[1], ic_log[2]], alpha=0.2, color=:blue, label="IC 95% Log√≠stica")
vspan!([ic_tree[1], ic_tree[2]], alpha=0.2, color=:green, label="IC 95% √Årbol")

# Opcional: agregar l√≠neas verticales para mayor claridad
vline!([ic_log[1]], line=:dash, color=:blue, alpha=0.5, label="")
vline!([ic_log[2]], line=:dash, color=:blue, alpha=0.5, label="")
vline!([ic_tree[1]], line=:dash, color=:green, alpha=0.5, label="")
vline!([ic_tree[2]], line=:dash, color=:green, alpha=0.5, label="")
# Evoluci√≥n accuracy
plot(1:n_boot, accs_log, label="Log√≠stica", color=:blue,
    xlabel="Iteraci√≥n Bootstrap", ylabel="Accuracy",
    title="Evoluci√≥n del Accuracy")
plot!(1:n_boot, accs_tree, label="√Årbol", color=:green)


# =============================
# 2. Modelo con un √∫nico √°rbol
# =============================
tree = DecisionTree.DecisionTreeClassifier(max_depth=3)
DecisionTree.fit!(tree, X_matrix, y_vector)
y_pred = DecisionTree.predict(tree, X_matrix)
acc_single = mean(y_pred .== y_vector)

println("Accuracy de un solo √°rbol: ", round(acc_single, digits=3))

# =============================
# 3. Bagging manual (bootstrap)
# =============================
function bagging(X, y; B=50, max_depth=3)
    n = size(X,1)
    preds = zeros(B, n)  # guardar predicciones de cada √°rbol

    for b in 1:B
        idxs = sample(1:n, n, replace=true)   # remuestreo bootstrap
        Xb = X[idxs, :]
        yb = y[idxs]

        tree_b = DecisionTree.DecisionTreeClassifier(max_depth=max_depth)
        DecisionTree.fit!(tree_b, Xb, yb)

        preds[b, :] = DecisionTree.predict(tree_b, X)   # predecimos sobre todo X
    end

    # Promedio de predicciones -> mayor√≠a
    y_final = [mean(preds[:,i]) >= 0.5 ? 1 : 0 for i in 1:n]
    acc = mean(y_final .== y)

    return acc, y_final
end

# =============================
# 4. Evaluar Bagging con distintos n√∫meros de √°rboles
# =============================
n_trees = 1:50
accs_bagging = [bagging(X_matrix, y_vector, B=b)[1] for b in n_trees]

# =============================
# 5. üìå Clasificaci√≥n final para un n√∫mero espec√≠fico de √°rboles
# =============================
acc_30, y_final_30 = bagging(X_matrix, y_vector, B=30)

n_inundados = count(==(1), y_final_30)
n_no_inundados = count(==(0), y_final_30)

println("\nResultados Bagging con 30 √°rboles:")
println("Accuracy: ", round(acc_30, digits=3))
println("Inundados (1): ", n_inundados)
println("No Inundados (0): ", n_no_inundados)

# =============================
# 5. Visualizaci√≥n
# =============================
plot(n_trees, accs_bagging,
     xlabel="N√∫mero de √°rboles en Bagging",
     ylabel="Accuracy",
     title="Bagging en predicci√≥n de inundaciones",
     label="Bagging (promedio)",
     lw=2, color=:blue)

hline!([acc_single], label="Un solo √°rbol", color=:red, lw=2, ls=:dash)




